{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c6f6ba",
   "metadata": {},
   "source": [
    "   Music data recognition\n",
    "   GTZAN Dataset - Music Genre Classification, Exracted_music_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d04758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "from itertools import cycle\n",
    "sns.set_theme(style='white', palette=None)\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826889dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing pop\n",
      "\n",
      "Processing metal\n",
      "\n",
      "Processing disco\n",
      "\n",
      "Processing blues\n",
      "\n",
      "Processing reggae\n",
      "\n",
      "Processing classical\n",
      "\n",
      "Processing rock\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '/Users/anagha/Desktop/Project/Data/genres_original'\n",
    "JSON_PATH = 'data.json'\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30   # measured in seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "    #dictionary to store  data\n",
    "    data = {\n",
    "        'mapping': [],\n",
    "        'mfcc': [],\n",
    "        'labels':[]\n",
    "    }\n",
    "    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment / hop_length)\n",
    "    # loop through all the genres\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        \n",
    "        #ensure that we're not at the root level\n",
    "        if dirpath is not dataset_path:\n",
    "            \n",
    "            # save the semantic level\n",
    "            dir_path_components = dirpath.split('/') #genre/blue => ['genre', 'blues']\n",
    "            semantic_label = dir_path_components[-1]\n",
    "            data['mapping'].append(semantic_label)\n",
    "            print('\\nProcessing {}'.format(semantic_label))\n",
    "            # process files for a specific genre \n",
    "            for f in filenames:\n",
    "                #load audio file\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                try:\n",
    "                    signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "                    # process segments extracting mfcc and storing data\n",
    "                    for s in range(num_segments):\n",
    "                        start_sample = num_samples_per_segment * s  # s=0 -> 0\n",
    "                        finish_sample = start_sample + num_samples_per_segment  # s=0 -> num_samples_per_segment\n",
    "                        \n",
    "                        # store mfcc for segment if it hasthe expected length\n",
    "                        mfcc = librosa.feature.mfcc(y=signal[start_sample:finish_sample],\n",
    "                                                    sr=sr,\n",
    "                                                    n_fft=n_fft,\n",
    "                                                    hop_length=hop_length,\n",
    "                                                    n_mfcc=n_mfcc)\n",
    "                        mfcc = mfcc.T\n",
    "                        if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                            data['mfcc'].append(mfcc.tolist())\n",
    "                            data['labels'].append(i - 1)\n",
    "#                             print('{}, segment:{}'.format(file_path, s+1))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    with open(json_path, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "                    \n",
    "                    \n",
    "if __name__ == \"__main__\":\n",
    "    save_mfcc(DATASET_PATH, JSON_PATH, num_segments=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8dae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "for genre in genres:\n",
    "    audio_files = glob(f'/Users/anagha/Desktop/Project/Data/genres_original/{genre}/*.wav')\n",
    "    if len(audio_files) > 0:\n",
    "        audio_file = audio_files[0]  # Select the first audio file\n",
    "        print(f\"Click to Play {genre.capitalize()}\")\n",
    "        # Play the audio file\n",
    "        ipd.display(ipd.Audio(audio_file))\n",
    "    else:\n",
    "        print(f\"No audio files found for genre: {genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'metal', 'jazz','pop', 'reggae', 'rock']\n",
    "fig, axs = plt.subplots(len(genres), 3, figsize=(20, 60))\n",
    "\n",
    "for i, genre in enumerate(genres):\n",
    "    audio_files = glob(f'/Users/anagha/Desktop/Project/Data/genres_original/{genre}/*.wav')\n",
    "    if len(audio_files) > 0:\n",
    "        audio_file = audio_files[0]\n",
    "\n",
    "        # Raw Audio\n",
    "        y, sr = librosa.load(audio_file)\n",
    "        axs[i, 0].plot(np.arange(len(y)) / sr, y, lw=1)\n",
    "        axs[i, 0].set_title(f\"Raw Audio - {genre}\")\n",
    "        axs[i, 0].set_xlabel('Time (s)')\n",
    "        axs[i, 0].set_ylabel('Amplitude')\n",
    "\n",
    "        # Trimmed Audio\n",
    "        y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "        pd.Series(y_trimmed).plot(ax=axs[i, 1], lw=1, title=f\"Raw Audio Trimmed ({genre})\", color=color_pal[1])\n",
    "\n",
    "        # Spectrogram\n",
    "        D = librosa.stft(y)\n",
    "        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "        img = librosa.display.specshow(S_db, x_axis='time', y_axis='log', ax=axs[i, 2])\n",
    "        axs[i, 2].set_title(f'Spectrogram Example ({genre.capitalize()})', fontsize=15)\n",
    "        fig.colorbar(img, ax=axs[i, 2], format='%+2.0f dB')\n",
    "\n",
    "    else:\n",
    "        for j in range(3):\n",
    "            axs[i, j].axis('off')\n",
    "        axs[i, 0].text(0.5, 0.5, f\"No audio files found for genre: {genre}\", horizontalalignment='center', verticalalignment='center', transform=axs[i, 0].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anagha S\n",
    "# anaghasudarshan23@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "for genre in genres:\n",
    "    audio_files = glob(f'/Users/anagha/Desktop/Project/Data/genres_original/{genre}/*.wav')\n",
    "    if len(audio_files) > 0:\n",
    "        audio_file = audio_files[0]  # Select the first audio file\n",
    "\n",
    "        y, sr = librosa.load(audio_file)\n",
    "\n",
    "        # Compute the Mel spectrogram\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128 * 2)\n",
    "        S_db_mel = librosa.amplitude_to_db(S, ref=np.max)\n",
    "\n",
    "        # Plot the Mel spectrogram\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        img = librosa.display.specshow(S_db_mel, x_axis='time', y_axis='log', ax=ax)\n",
    "        ax.set_title(f'Mel Spectrogram - {genre.capitalize()}', fontsize=20)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No audio files found for genre: {genre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e28c6",
   "metadata": {},
   "source": [
    "Machine Learning\n",
    "\n",
    "Load Data:\n",
    "The load_data function reads a dataset from a JSON file containing MFCC (Mel-frequency cepstral coefficients) features and their corresponding labels. MFCCs are commonly used in audio processing for representing the characteristics of audio signals.\n",
    "\n",
    "Split Data:\n",
    "The loaded data is split into training and testing sets using the train_test_split function from scikit-learn. This is a common practice in machine learning to evaluate the model's performance on unseen data.\n",
    "\n",
    "Build Neural Network:\n",
    "The neural network model is constructed using the Sequential API in Keras. This model consists of several layers:\n",
    "\n",
    "Input Layer:\n",
    "A Flatten layer is used to flatten the input MFCC data into a one-dimensional array. Dense Layers: Three dense (fully connected) layers with ReLU activation functions are added. These layers learn complex patterns in the data.\n",
    "\n",
    "Dropout Layers:\n",
    "Dropout layers are added after each dense layer to prevent overfitting. They randomly drop a fraction of the neurons during training. Output Layer: The output layer is a dense layer with a softmax activation function, which is suitable for multi-class classification problems. It outputs the probability distribution over the classes.\n",
    "\n",
    "Compile Model:\n",
    "The model is compiled using the Adam optimizer, sparse categorical crossentropy loss function, and accuracy metric. This prepares the model for training by specifying how it should learn from the data and evaluate its performance.\n",
    "\n",
    "Train Model:\n",
    "The model is trained using the fit method. The training data (X_train, y_train) is used for training, and the validation data (X_test, y_test) is used to evaluate the model's performance after each epoch. The batch_size parameter specifies the number of samples per gradient update, and the epochs parameter specifies the number of training iterations.\n",
    "\n",
    "Plot Training History:\n",
    "The plot_history function is used to visualize the training history, including the training and validation accuracy and loss over epochs. This helps to understand how the model is learning and whether it is overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae42df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/anagha/Desktop/Project/data_50.json'\n",
    "def load_data(data_path):\n",
    "    '''Loads training dataset from json file.\n",
    "    \n",
    "        :param data_path (str): Path to json file containing data\n",
    "        :return X (ndarray): Inputs\n",
    "        :return y (ndarray): Targets\n",
    "    '''\n",
    "    with open(data_path, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    \n",
    "    X = np.array(data['mfcc'])\n",
    "    y = np.array(data['labels'])\n",
    "    print(\"Data succesfully loaded!\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=('Accuracy eval', 'Error eval'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['accuracy']) + 1)),\n",
    "                             y=history.history['accuracy'], mode='lines', name='train accuracy'),\n",
    "                  row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_accuracy']) + 1)),\n",
    "                             y=history.history['val_accuracy'], mode='lines', name='test accuracy'),\n",
    "                  row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Accuracy', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Epoch', row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['loss']) + 1)),\n",
    "                             y=history.history['loss'], mode='lines', name='train error'),\n",
    "                  row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_loss']) + 1)),\n",
    "                             y=history.history['val_loss'], mode='lines', name='test error'),\n",
    "                  row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Error', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Epoch', row=2, col=1)\n",
    "\n",
    "    fig.update_layout(height=800, showlegend=True)\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #load data\n",
    "    X, y = load_data(DATA_PATH)\n",
    "    \n",
    "    # Create train/tets split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    # Build network topology\n",
    "    model = keras.Sequential([\n",
    "        # input layer\n",
    "        keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
    "        # 1st dense layer\n",
    "        keras.layers.Dense(512, input_dim= X_train.shape[1], activation='relu'),\n",
    "        # 2nd dense layer\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        # 3rd dense layer\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        # output layer\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=100)\n",
    "    \n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5dd83",
   "metadata": {},
   "source": [
    "In deep learning, particularly in the context of training neural networks, the terms \"loss,\" \"accuracy,\" \"validation loss,\" and \"validation accuracy\" are commonly used metrics to evaluate the performance of a model during training and validation. Here's a brief explanation of each:\n",
    "\n",
    "Loss: The loss, often represented as a single scalar value, indicates how well the model's predictions match the actual ground truth labels in the training data. It measures the error between the predicted output and the actual target values. The goal during training is to minimize this loss, as a lower loss indicates better alignment between predictions and actual labels.\n",
    "\n",
    "Accuracy: Accuracy is a metric that measures the proportion of correctly classified examples out of the total number of examples. It is usually expressed as a percentage. For example, an accuracy of 0.85 (or 85%) means that the model correctly predicted 85% of the examples in the dataset.\n",
    "\n",
    "Validation Loss: During training, a portion of the data (validation set) is typically set aside to evaluate the model's performance on unseen data. The validation loss is the loss calculated on this validation set. It helps to assess how well the model generalizes to new, unseen data. A decreasing validation loss indicates that the model is learning and improving its performance.\n",
    "\n",
    "Validation Accuracy: Similar to validation loss, validation accuracy is the accuracy calculated on the validation set. It provides a measure of how well the model is performing on unseen data. A high validation accuracy indicates that the model generalizes well to new data.\n",
    "\n",
    "In summary, the loss is a measure of how well the model is performing on the training data, while accuracy and validation metrics provide insights into its performance on both the training and unseen validation data. Lower loss and higher accuracy and validation metrics generally indicate better model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7853a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "X, y = load_data(DATA_PATH)\n",
    "\n",
    "# Create train/tets split\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the remaining data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=42)\n",
    "\n",
    "# Build network topology\n",
    "model = keras.Sequential([\n",
    "    # input layer\n",
    "    keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
    "    # 1st dense layer\n",
    "    keras.layers.Dense(512, input_dim= X_train.shape[1], activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    # 2nd dense layer\n",
    "    keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    # 3rd dense layer\n",
    "    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    # output layer\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,          # Stop after no improvement for 10 epochs\n",
    "    restore_best_weights=True  # Restore weights to the best model\n",
    ")\n",
    "\n",
    "# Modify model.fit to include the early stopping callback\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping]  # Pass the callback to the callbacks argument\n",
    ")\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b140a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, y):\n",
    "   X = X[np.newaxis, ...]\n",
    "   # prediction = [[0.1, 0.2, ...]]\n",
    "   prediction = model.predict(X)   # X-> (1, 130, 13, 1)\n",
    "   \n",
    "   # extract index with max value\n",
    "   predicted_index = np.argmax(prediction, axis=1)  # [4]\n",
    "   print(\"Expected index: {}, Predicted index: {}\".format(y, predicted_index))\n",
    "# make prediction on a sample\n",
    "\n",
    "# sample 1\n",
    "X = X_test[30]\n",
    "y = y_test[30]\n",
    "print('First sample')\n",
    "predict(model, X, y)\n",
    "\n",
    "# sample 2\n",
    "X = X_test[60]\n",
    "y = y_test[60]\n",
    "print('Second sample')\n",
    "predict(model, X, y)\n",
    "\n",
    "# sample 3\n",
    "X = X_test[100]\n",
    "y = y_test[100]\n",
    "print('Third sample')\n",
    "predict(model, X, y)\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d33635",
   "metadata": {},
   "source": [
    "model achieved an accuracy of 76.4%\n",
    "music genre classification,common task in music data recognition, achieving an accuracy above 70% is often considered acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a254cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
